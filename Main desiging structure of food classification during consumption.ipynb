{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxL0j3GaUPFH",
        "outputId": "aa65d6ad-cc90-4ffd-d637-465814b20fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# connect drive :)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSsxcerfS_aA"
      },
      "outputs": [],
      "source": [
        "# https://www.freecodecamp.org/news/handling-overfitting-in-deep-learning-models/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdfFPXSpiX1W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X_s4aSNieku",
        "outputId": "490fd1ec-e0b2-4b87-dbbe-efd19cb53076"
      },
      "outputs": [],
      "source": [
        "#read folder\n",
        "data_dir_path = os.listdir(\"/content/drive/MyDrive/Data_set\")\n",
        "print(data_dir_path)\n",
        "path, dirs, files = next(os.walk(\"/content/drive/MyDrive/Data_set\"))\n",
        "file_count = len(files)\n",
        "print(file_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "GkATixaNi2_4",
        "outputId": "558969bc-02b2-4e41-be80-44e53f34a088"
      },
      "outputs": [],
      "source": [
        "#make new Directory\n",
        "original_data_dir = \"/content/drive/MyDrive/Data_set\"\n",
        "base_dir = \"/content/drive/MyDrive/Data_set/new_data_set\"\n",
        "os.mkdir(base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc1NT8sajMLD"
      },
      "outputs": [],
      "source": [
        "#create three folders (train and validation and Test)\n",
        "base_dir = \"/content/drive/MyDrive/Data_set/new_data_set\"\n",
        "train_dir = os.path.join(base_dir,'train')\n",
        "os.mkdir(train_dir)\n",
        "\n",
        "validation_dir = os.path.join(base_dir,\"validation\")\n",
        "os.mkdir(validation_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I6_ZcjxAYVG"
      },
      "outputs": [],
      "source": [
        "Test_dir = os.path.join(base_dir,\"Test\")\n",
        "os.mkdir(Test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "W8oQ7VWtjyon",
        "outputId": "226db7f1-371b-44d5-99c4-c2dfe11831e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Test_zereshkpolo = os.path.join(Test_dir,\"zereshkpolo\")\\nos.mkdir(Test_zereshkpolo)\\n\\nTest_salad_makarani = os.path.join(Test_dir,\"salad_makarani\")\\nos.mkdir(Test_salad_makarani)\\n\\nTest_sabzipolo = os.path.join(Test_dir,\"sabzipolo\")\\nos.mkdir(Test_sabzipolo)\\n\\nTest_pasta = os.path.join(Test_dir,\"pasta\")\\nos.mkdir(Test_pasta)\\n\\nTest_olvie = os.path.join(Test_dir,\"olvie\")\\nos.mkdir(Test_olvie)\\n\\nTest_makarani = os.path.join(Test_dir,\"makarani\")\\nos.mkdir(Test_makarani)\\n\\nTest_lazania = os.path.join(Test_dir,\"lazania\")\\nos.mkdir(Test_lazania)\\n\\nTest_kotlet_ghoje_khyarshor = os.path.join(Test_dir,\"kotlet_ghoje_khyarshor\")\\nos.mkdir(Test_kotlet_ghoje_khyarshor)\\n\\nTest_koko_sabzi = os.path.join(Test_dir,\"koko_sabzi\")\\nos.mkdir(Test_koko_sabzi)\\n\\nTest_khorak_lobia = os.path.join(Test_dir,\"khorak_lobia\")\\nos.mkdir(Test_khorak_lobia)\\n\\nTest_kashkbademjan = os.path.join(Test_dir,\"kashkbademjan\")\\nos.mkdir(Test_kashkbademjan)\\n\\nTest_ghormesabzi = os.path.join(Test_dir,\"ghormesabzi\")\\nos.mkdir(Test_ghormesabzi)\\n\\nTest_gheyme = os.path.join(Test_dir,\"gheyme\")\\nos.mkdir(Test_gheyme)\\n\\nTest_adaspolo_ab_mast = os.path.join(Test_dir,\"adaspolo_ab_mast\")\\nos.mkdir(Test_adaspolo_ab_mast)\\n\\nTest_abgosht = os.path.join(Test_dir,\"abgosht\")\\nos.mkdir(Test_abgosht)\\n\\nTest_kababtabeii = os.path.join(Test_dir,\"kababtabeii\")\\nos.mkdir(Test_kababtabeii)'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# under the folder create fifteen folder\n",
        "#train\n",
        "\n",
        "train_zereshkpolo = os.path.join(train_dir,\"zereshkpolo\")\n",
        "os.mkdir(train_zereshkpolo)\n",
        "\n",
        "train_salad_makarani = os.path.join(train_dir,\"salad_makarani\")\n",
        "os.mkdir(train_salad_makarani)\n",
        "\n",
        "train_sabzipolo = os.path.join(train_dir,\"sabzipolo\")\n",
        "os.mkdir(train_sabzipolo)\n",
        "\n",
        "train_pasta = os.path.join(train_dir,\"pasta\")\n",
        "os.mkdir(train_pasta)\n",
        "\n",
        "train_olvie = os.path.join(train_dir,\"olvie\")\n",
        "os.mkdir(train_olvie)\n",
        "\n",
        "train_makarani = os.path.join(train_dir,\"makarani\")\n",
        "os.mkdir(train_makarani)\n",
        "\n",
        "train_lazania = os.path.join(train_dir,\"lazania\")\n",
        "os.mkdir(train_lazania)\n",
        "\n",
        "train_kotlet_ghoje_khyarshor = os.path.join(train_dir,\"kotlet_ghoje_khyarshor\")\n",
        "os.mkdir(train_kotlet_ghoje_khyarshor)\n",
        "\n",
        "train_koko_sabzi = os.path.join(train_dir,\"koko_sabzi\")\n",
        "os.mkdir(train_koko_sabzi)\n",
        "\n",
        "train_khorak_lobia = os.path.join(train_dir,\"khorak_lobia\")\n",
        "os.mkdir(train_khorak_lobia)\n",
        "\n",
        "train_kashkbademjan = os.path.join(train_dir,\"kashkbademjan\")\n",
        "os.mkdir(train_kashkbademjan)\n",
        "\n",
        "train_ghormesabzi = os.path.join(train_dir,\"ghormesabzi\")\n",
        "os.mkdir(train_ghormesabzi)\n",
        "\n",
        "train_gheyme = os.path.join(train_dir,\"gheyme\")\n",
        "os.mkdir(train_gheyme)\n",
        "\n",
        "train_adaspolo_ab_mast = os.path.join(train_dir,\"adaspolo_ab_mast\")\n",
        "os.mkdir(train_adaspolo_ab_mast)\n",
        "\n",
        "train_zereshkpolo = os.path.join(train_dir,\"abgosht\")\n",
        "os.mkdir(train_zereshkpolo)\n",
        "\n",
        "train_kababtabeii = os.path.join(train_dir,\"kababtabeii\")\n",
        "os.mkdir(train_kababtabeii)\n",
        "\n",
        "#=============================================================================\n",
        "# validation\n",
        "validation_zereshkpolo = os.path.join(validation_dir,\"zereshkpolo\")\n",
        "os.mkdir(validation_zereshkpolo)\n",
        "\n",
        "validation_salad_makarani = os.path.join(validation_dir,\"salad_makarani\")\n",
        "os.mkdir(validation_salad_makarani)\n",
        "\n",
        "validation_sabzipolo = os.path.join(validation_dir,\"sabzipolo\")\n",
        "os.mkdir(validation_sabzipolo)\n",
        "\n",
        "validation_pasta = os.path.join(validation_dir,\"pasta\")\n",
        "os.mkdir(validation_pasta)\n",
        "\n",
        "validation_olvie = os.path.join(validation_dir,\"olvie\")\n",
        "os.mkdir(validation_olvie)\n",
        "\n",
        "validation_makarani = os.path.join(validation_dir,\"makarani\")\n",
        "os.mkdir(validation_makarani)\n",
        "\n",
        "validation_lazania = os.path.join(validation_dir,\"lazania\")\n",
        "os.mkdir(validation_lazania)\n",
        "\n",
        "validation_kotlet_ghoje_khyarshor = os.path.join(validation_dir,\"kotlet_ghoje_khyarshor\")\n",
        "os.mkdir(validation_kotlet_ghoje_khyarshor)\n",
        "\n",
        "validation_koko_sabzi = os.path.join(validation_dir,\"koko_sabzi\")\n",
        "os.mkdir(validation_koko_sabzi)\n",
        "\n",
        "validation_khorak_lobia = os.path.join(validation_dir,\"khorak_lobia\")\n",
        "os.mkdir(validation_khorak_lobia)\n",
        "\n",
        "validation_kashkbademjan = os.path.join(validation_dir,\"kashkbademjan\")\n",
        "os.mkdir(validation_kashkbademjan)\n",
        "\n",
        "validation_ghormesabzi = os.path.join(validation_dir,\"ghormesabzi\")\n",
        "os.mkdir(validation_ghormesabzi)\n",
        "\n",
        "validation_gheyme = os.path.join(validation_dir,\"gheyme\")\n",
        "os.mkdir(validation_gheyme)\n",
        "\n",
        "validation_adaspolo_ab_mast = os.path.join(validation_dir,\"adaspolo_ab_mast\")\n",
        "os.mkdir(validation_adaspolo_ab_mast)\n",
        "\n",
        "validation_abgosht = os.path.join(validation_dir,\"abgosht\")\n",
        "os.mkdir(validation_abgosht)\n",
        "\n",
        "validation_kababtabeii = os.path.join(validation_dir,\"kababtabeii\")\n",
        "os.mkdir(validation_kababtabeii)\n",
        "\n",
        "#============================================================================\n",
        "# Test\n",
        "\n",
        "Test_zereshkpolo = os.path.join(Test_dir,\"zereshkpolo\")\n",
        "os.mkdir(Test_zereshkpolo)\n",
        "\n",
        "Test_salad_makarani = os.path.join(Test_dir,\"salad_makarani\")\n",
        "os.mkdir(Test_salad_makarani)\n",
        "\n",
        "Test_sabzipolo = os.path.join(Test_dir,\"sabzipolo\")\n",
        "os.mkdir(Test_sabzipolo)\n",
        "\n",
        "Test_pasta = os.path.join(Test_dir,\"pasta\")\n",
        "os.mkdir(Test_pasta)\n",
        "\n",
        "Test_olvie = os.path.join(Test_dir,\"olvie\")\n",
        "os.mkdir(Test_olvie)\n",
        "\n",
        "Test_makarani = os.path.join(Test_dir,\"makarani\")\n",
        "os.mkdir(Test_makarani)\n",
        "\n",
        "Test_lazania = os.path.join(Test_dir,\"lazania\")\n",
        "os.mkdir(Test_lazania)\n",
        "\n",
        "Test_kotlet_ghoje_khyarshor = os.path.join(Test_dir,\"kotlet_ghoje_khyarshor\")\n",
        "os.mkdir(Test_kotlet_ghoje_khyarshor)\n",
        "\n",
        "Test_koko_sabzi = os.path.join(Test_dir,\"koko_sabzi\")\n",
        "os.mkdir(Test_koko_sabzi)\n",
        "\n",
        "Test_khorak_lobia = os.path.join(Test_dir,\"khorak_lobia\")\n",
        "os.mkdir(Test_khorak_lobia)\n",
        "\n",
        "Test_kashkbademjan = os.path.join(Test_dir,\"kashkbademjan\")\n",
        "os.mkdir(Test_kashkbademjan)\n",
        "\n",
        "Test_ghormesabzi = os.path.join(Test_dir,\"ghormesabzi\")\n",
        "os.mkdir(Test_ghormesabzi)\n",
        "\n",
        "Test_gheyme = os.path.join(Test_dir,\"gheyme\")\n",
        "os.mkdir(Test_gheyme)\n",
        "\n",
        "Test_adaspolo_ab_mast = os.path.join(Test_dir,\"adaspolo_ab_mast\")\n",
        "os.mkdir(Test_adaspolo_ab_mast)\n",
        "\n",
        "Test_abgosht = os.path.join(Test_dir,\"abgosht\")\n",
        "os.mkdir(Test_abgosht)\n",
        "\n",
        "Test_kababtabeii = os.path.join(Test_dir,\"kababtabeii\")\n",
        "os.mkdir(Test_kababtabeii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoOOMLM5M5Vc"
      },
      "outputs": [],
      "source": [
        "# extract train 90% and validation 10% of remain data\n",
        "def split_data(SOURCE, TRAINING, VALIDATION, SPLIT_SIZE):\n",
        "    files = []\n",
        "    for filename in os.listdir(SOURCE):\n",
        "        file = SOURCE + filename\n",
        "        if os.path.getsize(file) > 0:\n",
        "            files.append(filename)\n",
        "        else:\n",
        "            print(filename + \" is zero length, so ignoring.\")\n",
        "\n",
        "    training_length = int(len(files) * SPLIT_SIZE)\n",
        "    valid_length = int(len(files) - training_length)\n",
        "    shuffled_set = random.sample(files, len(files))\n",
        "    training_set = shuffled_set[0:training_length]\n",
        "    valid_set = shuffled_set[training_length:]\n",
        "\n",
        "    for filename in training_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TRAINING + filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "    for filename in valid_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = VALIDATION + filename\n",
        "        copyfile(this_file, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E57cYO7vNBnN"
      },
      "outputs": [],
      "source": [
        "#extract test image 10%\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "def Test(src,dir):\n",
        "    source = src\n",
        "    dest = dir\n",
        "    files = os.listdir(source)\n",
        "    no_of_files = len(files) //10\n",
        "\n",
        "    for file_name in random.sample(files, no_of_files):\n",
        "        shutil.move(os.path.join(source, file_name), dest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lF_KbTNHjiw"
      },
      "outputs": [],
      "source": [
        "# Run to extract test image\n",
        "Test(\"/content/drive/MyDrive/Data_set/abgosht/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/abgosht/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/adaspolo-ab-mast/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/adaspolo_ab_mast/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/gheyme/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/gheyme/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/ghormesabzi/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/ghormesabzi/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/kababtabeii/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/kababtabeii/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/kashkbademjan/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/kashkbademjan/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/khorak-lobia/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/khorak_lobia/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/koko_sabzi/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/koko_sabzi/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/kotlet-ghoje-khyarshor/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/kotlet_ghoje_khyarshor/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/lazania/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/lazania/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/makarani/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/makarani/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/olvie/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/olvie/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/pasta/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/pasta/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/sabzipolo/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/sabzipolo/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/salad_makarani/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/salad_makarani/\")\n",
        "Test(\"/content/drive/MyDrive/Data_set/zereshkpolo/\",\"/content/drive/MyDrive/Data_set/new_data_set/Test/zereshkpolo/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7rkEa7_XlEW"
      },
      "outputs": [],
      "source": [
        "# set directory\n",
        "abgosht = \"/content/drive/MyDrive/Data_set/abgosht/\"\n",
        "training_abgosht = \"/content/drive/MyDrive/Data_set/new_data_set/train/abgosht/\"\n",
        "valid_abgosht = \"/content/drive/MyDrive/Data_set/new_data_set/validation/abgosht/\"\n",
        "\n",
        "adaspolo_ab_mast = \"/content/drive/MyDrive/Data_set/adaspolo-ab-mast/\"\n",
        "training_adaspolo_ab_mast = \"/content/drive/MyDrive/Data_set/new_data_set/train/adaspolo_ab_mast/\"\n",
        "valid_adaspolo_ab_mast = \"/content/drive/MyDrive/Data_set/new_data_set/validation/adaspolo_ab_mast/\"\n",
        "\n",
        "gheyme = \"/content/drive/MyDrive/Data_set/gheyme/\"\n",
        "training_gheyme = \"/content/drive/MyDrive/Data_set/new_data_set/train/gheyme/\"\n",
        "valid_gheyme = \"/content/drive/MyDrive/Data_set/new_data_set/validation/gheyme/\"\n",
        "\n",
        "ghormesabzi = \"/content/drive/MyDrive/Data_set/ghormesabzi/\"\n",
        "training_ghormesabzi = \"/content/drive/MyDrive/Data_set/new_data_set/train/ghormesabzi/\"\n",
        "valid_ghormesabzi = \"/content/drive/MyDrive/Data_set/new_data_set/validation/ghormesabzi/\"\n",
        "\n",
        "kababtabeii = \"/content/drive/MyDrive/Data_set/kababtabeii/\"\n",
        "training_kababtabeii = \"/content/drive/MyDrive/Data_set/new_data_set/train/kababtabeii/\"\n",
        "valid_kababtabeii = \"/content/drive/MyDrive/Data_set/new_data_set/validation/kababtabeii/\"\n",
        "\n",
        "kashkbademjan = \"/content/drive/MyDrive/Data_set/kashkbademjan/\"\n",
        "training_kashkbademjan = \"/content/drive/MyDrive/Data_set/new_data_set/train/kashkbademjan/\"\n",
        "valid_kashkbademjan = \"/content/drive/MyDrive/Data_set/new_data_set/validation/kashkbademjan/\"\n",
        "\n",
        "khorak_lobia = \"/content/drive/MyDrive/Data_set/khorak-lobia/\"\n",
        "training_khorak_lobia = \"/content/drive/MyDrive/Data_set/new_data_set/train/khorak_lobia/\"\n",
        "valid_khorak_lobia = \"/content/drive/MyDrive/Data_set/new_data_set/validation/khorak_lobia/\"\n",
        "\n",
        "koko_sabzi = \"/content/drive/MyDrive/Data_set/koko_sabzi/\"\n",
        "training_koko_sabzi = \"/content/drive/MyDrive/Data_set/new_data_set/train/koko_sabzi/\"\n",
        "valid_koko_sabzi = \"/content/drive/MyDrive/Data_set/new_data_set/validation/koko_sabzi/\"\n",
        "\n",
        "kotlet_ghoje_khyarshor = \"/content/drive/MyDrive/Data_set/kotlet-ghoje-khyarshor/\"\n",
        "training_kotlet_ghoje_khyarshor = \"/content/drive/MyDrive/Data_set/new_data_set/train/kotlet_ghoje_khyarshor/\"\n",
        "valid_kotlet_ghoje_khyarshor = \"/content/drive/MyDrive/Data_set/new_data_set/validation/kotlet_ghoje_khyarshor/\"\n",
        "\n",
        "lazania = \"/content/drive/MyDrive/Data_set/lazania/\"\n",
        "training_lazania = \"/content/drive/MyDrive/Data_set/new_data_set/train/lazania/\"\n",
        "valid_lazania = \"/content/drive/MyDrive/Data_set/new_data_set/validation/lazania/\"\n",
        "\n",
        "makarani = \"/content/drive/MyDrive/Data_set/makarani/\"\n",
        "training_makarani = \"/content/drive/MyDrive/Data_set/new_data_set/train/makarani/\"\n",
        "valid_makarani = \"/content/drive/MyDrive/Data_set/new_data_set/validation/makarani/\"\n",
        "\n",
        "olvie = \"/content/drive/MyDrive/Data_set/olvie/\"\n",
        "training_olvie = \"/content/drive/MyDrive/Data_set/new_data_set/train/olvie/\"\n",
        "valid_olvie = \"/content/drive/MyDrive/Data_set/new_data_set/validation/olvie/\"\n",
        "\n",
        "pasta = \"/content/drive/MyDrive/Data_set/pasta/\"\n",
        "training_pasta = \"/content/drive/MyDrive/Data_set/new_data_set/train/pasta/\"\n",
        "valid_pasta = \"/content/drive/MyDrive/Data_set/new_data_set/validation/pasta/\"\n",
        "\n",
        "sabzipolo = \"/content/drive/MyDrive/Data_set/sabzipolo/\"\n",
        "training_sabzipolo = \"/content/drive/MyDrive/Data_set/new_data_set/train/sabzipolo/\"\n",
        "valid_sabzipolo = \"/content/drive/MyDrive/Data_set/new_data_set/validation/sabzipolo/\"\n",
        "\n",
        "salad_makarani = \"/content/drive/MyDrive/Data_set/salad_makarani/\"\n",
        "training_salad_makarani = \"/content/drive/MyDrive/Data_set/new_data_set/train/salad_makarani/\"\n",
        "valid_salad_makarani = \"/content/drive/MyDrive/Data_set/new_data_set/validation/salad_makarani/\"\n",
        "\n",
        "zereshkpolo = \"/content/drive/MyDrive/Data_set/zereshkpolo/\"\n",
        "training_zereshkpolo = \"/content/drive/MyDrive/Data_set/new_data_set/train/zereshkpolo/\"\n",
        "valid_zereshkpolo = \"/content/drive/MyDrive/Data_set/new_data_set/validation/zereshkpolo/\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTQwozAAbhAw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "\n",
        "split_SIZE = 0.90\n",
        "\n",
        "split_data(abgosht , training_abgosht ,valid_abgosht , split_SIZE )\n",
        "split_data(adaspolo_ab_mast , training_adaspolo_ab_mast ,valid_adaspolo_ab_mast , split_SIZE )\n",
        "split_data(gheyme , training_gheyme ,valid_gheyme , split_SIZE )\n",
        "split_data(ghormesabzi , training_ghormesabzi ,valid_ghormesabzi , split_SIZE )\n",
        "split_data(kababtabeii , training_kababtabeii ,valid_kababtabeii , split_SIZE )\n",
        "split_data(kashkbademjan , training_kashkbademjan ,valid_kashkbademjan , split_SIZE )\n",
        "split_data(khorak_lobia , training_khorak_lobia ,valid_khorak_lobia , split_SIZE )\n",
        "split_data(koko_sabzi , training_koko_sabzi ,valid_koko_sabzi , split_SIZE )\n",
        "split_data(kotlet_ghoje_khyarshor , training_kotlet_ghoje_khyarshor ,valid_kotlet_ghoje_khyarshor , split_SIZE )\n",
        "split_data(lazania , training_lazania ,valid_lazania , split_SIZE )\n",
        "split_data(makarani , training_makarani ,valid_makarani , split_SIZE )\n",
        "split_data(olvie , training_olvie ,valid_olvie , split_SIZE )\n",
        "split_data(pasta , training_pasta ,valid_pasta , split_SIZE )\n",
        "split_data(sabzipolo , training_sabzipolo ,valid_sabzipolo , split_SIZE )\n",
        "split_data(salad_makarani , training_salad_makarani ,valid_salad_makarani , split_SIZE )\n",
        "split_data(zereshkpolo , training_zereshkpolo ,valid_zereshkpolo , split_SIZE )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "qP-Biu_BdXlC",
        "outputId": "58fd810a-a3b9-4ed8-a145-07dd298dfac4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.image import imread\n",
        "import pathlib\n",
        "\n",
        "image_folder = ['zereshkpolo', 'salad_makarani', 'sabzipolo', 'pasta', 'olvie', 'makarani', 'lazania', 'kotlet_ghoje_khyarshor', 'koko_sabzi', 'khorak_lobia', 'kashkbademjan', 'kababtabeii', 'ghormesabzi', 'gheyme', 'adaspolo_ab_mast', 'abgosht']\n",
        "nimgs ={}\n",
        "for i in image_folder:\n",
        "    nimages = len(os.listdir(\"/content/drive/MyDrive/Data_set/new_data_set/train/\"+i+\"/\"))\n",
        "    nimgs[i]=nimages\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.bar(range(len(nimgs)),list(nimgs.values()),align='center')\n",
        "plt.xticks(range(len(nimgs)),list(nimgs.keys()))\n",
        "plt.title(\"DISTRIBUTION IN DIFFERENT CLASSES IN TRAINING DATASETS\")\n",
        "plt.show()\n",
        "\n",
        "for i in['zereshkpolo', 'salad_makarani', 'sabzipolo', 'pasta', 'olvie', 'makarani', 'lazania', 'kotlet_ghoje_khyarshor', 'koko_sabzi', 'khorak_lobia', 'kashkbademjan', 'kababtabeii', 'ghormesabzi', 'gheyme', 'adaspolo_ab_mast', 'abgosht']:\n",
        "    print(\"training_{} images are :\".format(i)+str(len(os.listdir(\"/content/drive/MyDrive/Data_set/new_data_set/train/\"+i+\"/\"))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwB4kZ7FZ2Ql"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NsVOVqfMkeQx",
        "outputId": "4d843e52-ce79-467c-84e2-6a8e6fa35df0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.image import imread\n",
        "import pathlib\n",
        "\n",
        "image_folder = ['zereshkpolo', 'salad_makarani', 'sabzipolo', 'pasta', 'olvie', 'makarani', 'lazania', 'kotlet_ghoje_khyarshor', 'koko_sabzi', 'khorak_lobia', 'kashkbademjan', 'kababtabeii', 'ghormesabzi', 'gheyme', 'adaspolo_ab_mast', 'abgosht']\n",
        "nimgs ={}\n",
        "for i in image_folder:\n",
        "    nimages = len(os.listdir(\"/content/drive/MyDrive/Data_set/new_data_set/validation/\"+i+\"/\"))\n",
        "    nimgs[i]=nimages\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.bar(range(len(nimgs)),list(nimgs.values()),align='center')\n",
        "plt.xticks(range(len(nimgs)),list(nimgs.keys()))\n",
        "plt.title(\"DISTRIBUTION IN DIFFERENT CLASSES IN TRAINING DATASETS\")\n",
        "plt.show()\n",
        "\n",
        "for i in['zereshkpolo', 'salad_makarani', 'sabzipolo', 'pasta', 'olvie', 'makarani', 'lazania', 'kotlet_ghoje_khyarshor', 'koko_sabzi', 'khorak_lobia', 'kashkbademjan', 'kababtabeii', 'ghormesabzi', 'gheyme', 'adaspolo_ab_mast', 'abgosht']:\n",
        "    print(\"validation_{} images are :\".format(i)+str(len(os.listdir(\"/content/drive/MyDrive/Data_set/new_data_set/validation/\"+i+\"/\"))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "9Ex0aluvorMa",
        "outputId": "9365614b-c4ea-404a-b423-c174e11a4f6e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.image import imread\n",
        "import pathlib\n",
        "\n",
        "image_folder = ['zereshkpolo', 'salad_makarani', 'sabzipolo', 'pasta', 'olvie', 'makarani', 'lazania', 'kotlet_ghoje_khyarshor', 'koko_sabzi', 'khorak_lobia', 'kashkbademjan', 'kababtabeii', 'ghormesabzi', 'gheyme', 'adaspolo_ab_mast', 'abgosht']\n",
        "nimgs ={}\n",
        "for i in image_folder:\n",
        "    nimages = len(os.listdir(\"/content/drive/MyDrive/Data_set/new_data_set/Test/\"+i+\"/\"))\n",
        "    nimgs[i]=nimages\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.bar(range(len(nimgs)),list(nimgs.values()),align='center')\n",
        "plt.xticks(range(len(nimgs)),list(nimgs.keys()))\n",
        "plt.title(\"DISTRIBUTION IN DIFFERENT CLASSES IN TRAINING DATASETS\")\n",
        "plt.show()\n",
        "\n",
        "for i in['zereshkpolo', 'salad_makarani', 'sabzipolo', 'pasta', 'olvie', 'makarani', 'lazania', 'kotlet_ghoje_khyarshor', 'koko_sabzi', 'khorak_lobia', 'kashkbademjan', 'kababtabeii', 'ghormesabzi', 'gheyme', 'adaspolo_ab_mast', 'abgosht']:\n",
        "    print(\"Test_{} images are :\".format(i)+str(len(os.listdir(\"/content/drive/MyDrive/Data_set/new_data_set/Test/\"+i+\"/\"))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YOF-5IcMMuf"
      },
      "source": [
        "**start deep learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS1EChwklWnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRBIuY10lfC9"
      },
      "outputs": [],
      "source": [
        "img_width=400 ; img_height=400\n",
        "batch_size=16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye4D-pFwod5Y",
        "outputId": "f624c0ff-e9fa-40a3-e554-fed8f5a5ff9b"
      },
      "outputs": [],
      "source": [
        "TRAINING_DIR = \"/content/drive/MyDrive/Data_Food_Consumed/new/Train\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1/255.0,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   shear_range = 0.2,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode ='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(img_height, img_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThAKnWB6o16d",
        "outputId": "70bdf667-f483-4549-af0d-acef9af0622b"
      },
      "outputs": [],
      "source": [
        "VALIDATION_DIR = \"/content/drive/MyDrive/Data_Food_Consumed/new/Valid\"\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              target_size=(img_height, img_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7V_vsfXpBzt",
        "outputId": "ae66161c-99f2-4a53-e9f3-2bc6e50652bf"
      },
      "outputs": [],
      "source": [
        "img_width=640 ; img_height=340\n",
        "batch_size=16\n",
        "TEST_DIR = \"/content/drive/MyDrive/Data_set/new_data_set/Test/\"\n",
        "\n",
        "Test_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "\n",
        "Test_generator = Test_datagen.flow_from_directory(TEST_DIR,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  target_size=(img_height, img_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWkkAB8vpjWt"
      },
      "outputs": [],
      "source": [
        "# callbacks\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr=ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience= 10 , verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bcw5MXY1rNIY"
      },
      "outputs": [],
      "source": [
        "from keras.applications import resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ucbKQjpq7U9"
      },
      "outputs": [],
      "source": [
        "conv=resnet.ResNet50(weights=\"imagenet\",include_top=False,input_shape=(340 ,640,3))\n",
        "model=models.Sequential()\n",
        "model.add(conv)\n",
        "\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "model.add(layers.Dense(16, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_17DY_4ri0G",
        "outputId": "9a8ed63d-8064-43bd-8be3-55462904a593"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96-zcEKLsx2h"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='Adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics =['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26LOHxVN-hgS",
        "outputId": "1fa9eb53-b85e-435c-acdc-ec1ca8d53302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "674/674 [==============================] - 3820s 6s/step - loss: 2.1289 - accuracy: 0.3760 - val_loss: 8.9550 - val_accuracy: 0.0507 - lr: 0.0010\n",
            "Epoch 2/5\n",
            "674/674 [==============================] - 1300s 2s/step - loss: 0.9393 - accuracy: 0.6974 - val_loss: 4.8240 - val_accuracy: 0.4903 - lr: 0.0010\n",
            "Epoch 3/5\n",
            "674/674 [==============================] - 1291s 2s/step - loss: 0.5258 - accuracy: 0.8437 - val_loss: 0.8408 - val_accuracy: 0.7645 - lr: 0.0010\n",
            "Epoch 4/5\n",
            "674/674 [==============================] - 1301s 2s/step - loss: 0.3773 - accuracy: 0.8924 - val_loss: 0.5203 - val_accuracy: 0.8345 - lr: 0.0010\n",
            "Epoch 5/5\n",
            "674/674 [==============================] - 1307s 2s/step - loss: 0.2490 - accuracy: 0.9330 - val_loss: 0.3449 - val_accuracy: 0.9203 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_generator,\n",
        "                              epochs= 5,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator,\n",
        "                              callbacks = [reduce_lr]\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhtJpFJIFv59",
        "outputId": "dc70d0cb-688e-49af-c1ba-4b777391d252"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        }
      ],
      "source": [
        "model.save(\"/content/drive/MyDrive/Data_set/new_data_set/best_currect_cnn.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "9jyETj4qYj1F",
        "outputId": "9074f5e4-7bdb-4bcc-a01a-2201429cce09"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.image import imread\n",
        "import pathlib\n",
        "\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc))\n",
        "\n",
        "fig = plt.figure(figsize=(14,7))\n",
        "plt.plot(epochs, acc, 'r', label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', label=\"Validation Accuracy\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "30avj9UqYlWi",
        "outputId": "dac30c65-c8a5-41c6-95bb-16681991ef87"
      },
      "outputs": [],
      "source": [
        "fig2 = plt.figure(figsize=(14,7))\n",
        "plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApI2RN7ZIF0Y"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "from keras.models import load_model\n",
        "model = load_model(\"/content/drive/MyDrive/Data_set/new_data_set/best_currect_cnn.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkV1CE2pKEpK",
        "outputId": "d454451e-03be-4df6-abe5-1d2a45941c9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss is : 0.2871497869491577\n",
            "Test accuracy is :  0.9287499785423279\n"
          ]
        }
      ],
      "source": [
        "test_loss , test_acc = model.evaluate_generator(Test_generator , steps=50)\n",
        "print(\"Test loss is :\",test_loss)\n",
        "print(\"Test accuracy is : \", test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga7YWGUIQH0I"
      },
      "outputs": [],
      "source": [
        "X_test , Y_test = next(Test_generator)\n",
        "X_train , Y_train = next(train_generator)\n",
        "X_val , Y_val = next(validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAi1IHctUlvn"
      },
      "outputs": [],
      "source": [
        "X_train , Y_train = next(train_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieXOxh01jIta"
      },
      "outputs": [],
      "source": [
        "#train_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "ypred_train = model.predict(X_train)\n",
        "train_labels_p = np.argmax(ypred_train, axis=1)\n",
        "train_true = np.argmax(Y_train, axis=1)\n",
        "cm_train = confusion_matrix(train_true, train_labels_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl_SSn9SjaFz",
        "outputId": "2e8566bf-0490-4957-f024-b30cb99fad90"
      },
      "outputs": [],
      "source": [
        "print(cm_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBWJLv0tkXr8",
        "outputId": "d96d647e-7ec7-4e51-d40b-dc024186db45"
      },
      "outputs": [],
      "source": [
        "#performance train data\n",
        "#performance test data\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "Accuracy_train= accuracy_score(train_true, train_labels_p)\n",
        "MSE_train= mean_squared_error(train_true, train_labels_p,squared=True)\n",
        "RMSE_train= mean_squared_error(train_true, train_labels_p,squared=False)\n",
        "MAE_train= mean_absolute_error(train_true, train_labels_p)\n",
        "Precision_train= precision_score(train_true, train_labels_p,average=None)\n",
        "Sensitivity_train = recall_score(train_true, train_labels_p,average=None)\n",
        "F1_train= f1_score(train_true, train_labels_p,average=None)\n",
        "#train performance\n",
        "print(cm_train)\n",
        "print('Accuracy_train' + str(Accuracy_train))\n",
        "print('MSE_train' + str(MSE_train))\n",
        "print('RMSE_train' + str(RMSE_train))\n",
        "print('MAE_train' + str(MAE_train))\n",
        "print('Precision_train' + str(Precision_train))\n",
        "print('Sensitivity_train' + str(Sensitivity_train))\n",
        "print('F1_score_train' + str(F1_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBetjS0RUpIY",
        "outputId": "a8f29299-3639-4c00-eb0a-241296bfda42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16, 340, 640, 3)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSgkx-lEgtgM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "#test confusion_matrix\n",
        "ypred_test = model.predict(X_test)\n",
        "test_labels_p = np.argmax(ypred_test, axis=1)\n",
        "test_true = np.argmax(Y_test, axis=1)\n",
        "cm_test = confusion_matrix(test_true, test_labels_p)\n",
        "\n",
        "#validation confusion_matrix\n",
        "ypred_valid = model.predict(X_val)\n",
        "valid_labels_p = np.argmax(ypred_valid, axis=1)\n",
        "valid_true = np.argmax(Y_val, axis=1)\n",
        "cm_valid = confusion_matrix(valid_true, valid_labels_p)\n",
        "\n",
        "#train_confusion_matrix\n",
        "ypred_train = model.predict(X_train)\n",
        "train_labels_p = np.argmax(ypred_train, axis=1)\n",
        "train_true = np.argmax(Y_train, axis=1)\n",
        "cm_train = confusion_matrix(train_true, train_labels_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unGB8QbUh9qg",
        "outputId": "f51b02e2-10c9-45b8-fe92-078ad1b206e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#performance test data\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "Accuracy_test= accuracy_score(test_true, test_labels_p)\n",
        "MSE_test= mean_squared_error(test_true, test_labels_p,squared=True)\n",
        "RMSE_test= mean_squared_error(test_true, test_labels_p,squared=False)\n",
        "MAE_test= mean_absolute_error(test_true, test_labels_p)\n",
        "Precision_test= precision_score(test_true, test_labels_p,average=None)\n",
        "Sensitivity_test = recall_score(test_true, test_labels_p,average=None)\n",
        "F1_test= f1_score(test_true, test_labels_p,average=None)\n",
        "\n",
        "#performance valid data\n",
        "Accuracy_valid= accuracy_score(valid_true, valid_labels_p)\n",
        "MSE_valid= mean_squared_error(valid_true, valid_labels_p,squared=True)\n",
        "RMSE_valid= mean_squared_error(valid_true, valid_labels_p,squared=False)\n",
        "MAE_valid= mean_absolute_error(valid_true, valid_labels_p)\n",
        "Precision_valid= precision_score(valid_true, valid_labels_p,average=None)\n",
        "Sensitivity_valid = recall_score(valid_true, valid_labels_p,average=None)\n",
        "F1_valid= f1_score(valid_true, valid_labels_p,average=None)\n",
        "\n",
        "#performance train data\n",
        "Accuracy_train= accuracy_score(train_true, train_labels_p)\n",
        "MSE_train= mean_squared_error(train_true, train_labels_p,squared=True)\n",
        "RMSE_train= mean_squared_error(train_true, train_labels_p,squared=False)\n",
        "MAE_train= mean_absolute_error(train_true, train_labels_p)\n",
        "Precision_train= precision_score(train_true, train_labels_p,average=None)\n",
        "Sensitivity_train = recall_score(train_true, train_labels_p,average=None)\n",
        "F1_train= f1_score(train_true, train_labels_p,average=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jR6EKO8iMcm",
        "outputId": "de0e0abe-e28a-46c8-d997-879f46b0030f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 2 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 2 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 4]]\n",
            "Accuracy_test1.0\n",
            "MSE_test0.0\n",
            "RMSE_test0.0\n",
            "MAE_test0.0\n",
            "Precision_test[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Sensitivity_test[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "F1_score_test[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "#show performance\n",
        "#test performance\n",
        "print(cm_test)\n",
        "print('Accuracy_test' + str(Accuracy_test))\n",
        "print('MSE_test' + str(MSE_test))\n",
        "print('RMSE_test' + str(RMSE_test))\n",
        "print('MAE_test' + str(MAE_test))\n",
        "print('Precision_test' + str(Precision_test))\n",
        "print('Sensitivity_test' + str(Sensitivity_test))\n",
        "print('F1_score_test' + str(F1_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taxGtt5JigQb",
        "outputId": "8dcf9721-a711-40fe-9c5c-7cf5c34c2d0b"
      },
      "outputs": [],
      "source": [
        "#valid performance\n",
        "print(cm_valid)\n",
        "print('Accuracy_valid' + str(Accuracy_valid))\n",
        "print('MSE_valid' + str(MSE_valid))\n",
        "print('RMSE_valid' + str(RMSE_valid))\n",
        "print('MAE_valid' + str(MAE_valid))\n",
        "print('Precision_valid' + str(Precision_valid))\n",
        "print('Sensitivity_valid' + str(Sensitivity_valid))\n",
        "print('F1_score_valid' + str(F1_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09h8ehUkiocR",
        "outputId": "d39aba92-c7f5-4118-dca3-01e9b1d55a54"
      },
      "outputs": [],
      "source": [
        "#train performance\n",
        "print(cm_train)\n",
        "print('Accuracy_train' + str(Accuracy_train))\n",
        "print('MSE_train' + str(MSE_train))\n",
        "print('RMSE_train' + str(RMSE_train))\n",
        "print('MAE_train' + str(MAE_train))\n",
        "print('Precision_train' + str(Precision_train))\n",
        "print('Sensitivity_train' + str(Sensitivity_train))\n",
        "print('F1_score_train' + str(F1_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AkVAN8YpjDZp",
        "outputId": "f86e9563-b597-4250-aef8-36786b80cbac"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#Plot the confusion matrix. Set Normalize = True/False\n",
        "def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "      cm = np.around(cm, decimals=2)\n",
        "      cm[np.isnan(cm)] = 0.0\n",
        "      print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "      print('Confusion matrix, without normalization')\n",
        "      thresh = cm.max() / 2.\n",
        "      for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "        horizontalalignment=\"center\",\n",
        "        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        plt.tight_layout()\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "#Print the Target names\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "#shuffle=False\n",
        "target_names = []\n",
        "for key in train_generator.class_indices:\n",
        "    target_names.append(key)\n",
        "# print(target_names)\n",
        "#Confution Matrix\n",
        "Y_pred = model.predict_generator(train_generator)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print('Confusion Matrix')\n",
        "cm = confusion_matrix(train_generator.classes, y_pred)\n",
        "plot_confusion_matrix(cm, target_names, title='Confusion Matrix')\n",
        "#Print Classification Report\n",
        "print('Classification Report')\n",
        "print(classification_report(train_generator.classes, y_pred, target_names=target_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwJzUnEJaC2I",
        "outputId": "4c34d3b4-04df-4f8e-adeb-24f286d7df3d"
      },
      "outputs": [],
      "source": [
        "img_width=640 ; img_height=340\n",
        "batch_size=16\n",
        "TEST_DIR = \"/content/drive/MyDrive/Data_set/new_data_set/Test/\"\n",
        "\n",
        "Test_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "\n",
        "Test_generator = Test_datagen.flow_from_directory(TEST_DIR,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  target_size=(img_height, img_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzMy9ubZN2w0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#Plot the confusion matrix. Set Normalize = True/False\n",
        "def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(40,15))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "      cm = np.around(cm, decimals=2)\n",
        "      cm[np.isnan(cm)] = 0.0\n",
        "      print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "      print('Confusion matrix, without normalization')\n",
        "      thresh = cm.max() / 2.\n",
        "      for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "        horizontalalignment=\"center\",\n",
        "        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        plt.tight_layout()\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "#Print the Target names\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "#shuffle=False\n",
        "target_names = []\n",
        "for key in train_generator.class_indices:\n",
        "    target_names.append(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "CP2g8Y_3Nuo_",
        "outputId": "a29f2e08-4c1e-48df-c3f9-564a43cb8238"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(cm, target_names, title='Confusion Matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "8UgzNL9rOmk6",
        "outputId": "a6d2c3a3-50c7-4b84-8d87-9a9c0c630805"
      },
      "outputs": [],
      "source": [
        "#train_confusion_matrix\n",
        "train_labels_p = y_pred\n",
        "train_true = train_generator.classes\n",
        "cm_train = confusion_matrix(train_true, train_labels_p)\n",
        "print(cm_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbzaOkGCPRm4"
      },
      "outputs": [],
      "source": [
        "#performance train data\n",
        "Accuracy_train= accuracy_score(train_true, train_labels_p)\n",
        "MSE_train= mean_squared_error(train_true, train_labels_p,squared=True)\n",
        "RMSE_train= mean_squared_error(train_true, train_labels_p,squared=False)\n",
        "MAE_train= mean_absolute_error(train_true, train_labels_p)\n",
        "Precision_train= precision_score(train_true, train_labels_p,average=None)\n",
        "Sensitivity_train = recall_score(train_true, train_labels_p,average=None)\n",
        "F1_train= f1_score(train_true, train_labels_p,average=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaU8rmXRPXeZ",
        "outputId": "75fbf68b-d1dd-49ec-b9cf-62fe0bd3b7cc"
      },
      "outputs": [],
      "source": [
        "#train performance\n",
        "print(cm_train)\n",
        "print('Accuracy_train' + str(Accuracy_train))\n",
        "print('MSE_train' + str(MSE_train))\n",
        "print('RMSE_train' + str(RMSE_train))\n",
        "print('MAE_train' + str(MAE_train))\n",
        "print('Precision_train' + str(Precision_train))\n",
        "print('Sensitivity_train' + str(Sensitivity_train))\n",
        "print('F1_score_train' + str(F1_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQC66TYINaXe",
        "outputId": "16950bb2-db48-4919-a482-bc0bdaabd36a"
      },
      "outputs": [],
      "source": [
        "Test_generator.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZ0E15rIekRG"
      },
      "outputs": [],
      "source": [
        "#performance test data\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "Accuracy_test= accuracy_score(test_true, test_labels_p)\n",
        "MSE_test= mean_squared_error(test_true, test_labels_p,squared=True)\n",
        "RMSE_test= mean_squared_error(test_true, test_labels_p,squared=False)\n",
        "MAE_test= mean_absolute_error(test_true, test_labels_p)\n",
        "Precision_test= precision_score(test_true, test_labels_p,average=None)\n",
        "Sensitivity_test = recall_score(test_true, test_labels_p,average=None)\n",
        "F1_test= f1_score(test_true, test_labels_p,average=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQH6qLhqiHhS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "\n",
        "y_pred = y_pred\n",
        "y_true = train_generator.classes\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print('Classification Report')\n",
        "target_names = []\n",
        "for key in train_generator.class_indices:\n",
        "    target_names.append(key)\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True',\n",
        "           xlabel='Predicted')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\", fontsize=10, fontweight='bold')\n",
        "    plt.setp(ax.get_yticklabels(), fontsize=10, fontweight='bold')\n",
        "    plt.setp(ax.xaxis.get_label(), fontsize=10, fontweight='bold')\n",
        "    plt.setp(ax.yaxis.get_label(), fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "target_names=np.array(target_names)\n",
        "\n",
        "plot_confusion_matrix(y_true, y_pred, classes=target_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "plt.grid(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krB2ZIfNBYTJ"
      },
      "source": [
        "InceptionV3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3IZKvTCCAdu"
      },
      "outputs": [],
      "source": [
        "img_width=300 ; img_height=300\n",
        "batch_size=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je46HmkYB_Mo",
        "outputId": "309c0304-d3c4-4ac0-8458-d71f4499f45a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3248 images belonging to 16 classes.\n"
          ]
        }
      ],
      "source": [
        "TRAINING_DIR = \"/content/drive/MyDrive/Data_Food_Consumed/new/Train\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(img_height, img_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3e5A0I8B_Mq",
        "outputId": "5244e74a-5912-429b-ff21-af68775150dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 422 images belonging to 16 classes.\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_DIR = \"/content/drive/MyDrive/Data_Food_Consumed/new/Valid\"\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              target_size=(img_height, img_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSpGMVOLBaa3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import tensorflow as tf\n",
        "import matplotlib.image as img\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import collections\n",
        "from shutil import copy\n",
        "from shutil import copytree, rmtree\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5lYyyJNIBfTx",
        "outputId": "bda934de-1a1a-4fdc-d997-7a80a93d6d76"
      },
      "outputs": [],
      "source": [
        "# Let's use a pretrained Inceptionv3 model on subset of data with 11 food classes\n",
        "K.clear_session()\n",
        "\n",
        "\n",
        "inception = InceptionV3(weights='imagenet', include_top=False)\n",
        "x = inception.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128,activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "\n",
        "predictions = Dense(6,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=inception.input, outputs=predictions)\n",
        "model.compile(optimizer=SGD(lr=0.001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "checkpointer = ModelCheckpoint(filepath='best_model_6class.hdf5', verbose=1, save_best_only=True)\n",
        "csv_logger = CSVLogger('history_6class.log')\n",
        "\n",
        "\n",
        "history_11class = model.fit_generator(train_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    epochs=8,\n",
        "                    verbose=1,\n",
        "                    callbacks=[csv_logger, checkpointer])\n",
        "\n",
        "model.save('model_trained_6class.hdf5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EfficientNetB7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# connect drive :)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.image import imread\n",
        "import pathlib\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_width=600 ; img_height=600\n",
        "batch_size=16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAINING_DIR = \"/content/drive/MyDrive/New_data_set/1_train/\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1/255.0,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   shear_range = 0.2,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode ='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size= batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(img_height, img_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VALIDATION_DIR = \"/content/drive/MyDrive/New_data_set/validation/\"\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              target_size=(img_height, img_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# callbacks\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr=ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience= 10 , verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.applications.efficientnet import EfficientNetB7\n",
        "conv= EfficientNetB7(weights=\"imagenet\",include_top=False,input_shape=(456 ,456,3))\n",
        "model=models.Sequential()\n",
        "model.add(conv)\n",
        "model.trainable = False\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(32 , activation=\"relu\"))\n",
        "\n",
        "\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Dense(16, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    print(\"Device:\", tpu.master())\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
        "    strategy = tf.distribute.MirroredStrategy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "from keras.applications import EfficientNetB7\n",
        "IMG_SIZE = 600\n",
        "\n",
        "def build_model(num_classes):\n",
        "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    x = inputs\n",
        "    model = EfficientNetB7(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
        "\n",
        "    # Freeze the pretrained weights\n",
        "    model.trainable = False\n",
        "\n",
        "    # Rebuild top\n",
        "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    top_dropout_rate = 0.2\n",
        "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "    Dense1 = layers.Dense(512, activation='relu')(x)\n",
        "    Dense2 = layers.Dense(256, activation='relu')(Dense1)\n",
        "    Dense3 = layers.Dense(128, activation='relu')(Dense2)\n",
        "    Dense4 = layers.Dense(64, activation='relu')(Dense3)\n",
        "    Dense5 = layers.Dense(32, activation='relu')(Dense4)\n",
        "    outputs = layers.Dense(16, activation=\"softmax\", name=\"pred\")(Dense5)\n",
        "\n",
        "    # Compile\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = build_model(num_classes=16)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_hist(hist):\n",
        "    plt.plot(hist.history[\"accuracy\"])\n",
        "    plt.plot(hist.history[\"val_accuracy\"])\n",
        "    plt.title(\"model accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model = build_model(num_classes=16)\n",
        "    model.summary()\n",
        "\n",
        "epochs = 8  # @param {type: \"slider\", min:8, max:80}\n",
        "hist = model.fit(train_generator, epochs=epochs, validation_data=validation_generator, verbose=2)\n",
        "plot_hist(hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install efficientnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.applications import EfficientNetB7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv= EfficientNetB7(weights=\"imagenet\",include_top=False,input_shape=(400 ,400,3))\n",
        "model=models.Sequential()\n",
        "model.add(conv)\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Dense(16, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer='Adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics =['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(train_generator,\n",
        "                              epochs= 5,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator,\n",
        "                              callbacks = [reduce_lr]\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc))\n",
        "\n",
        "fig = plt.figure(figsize=(14,7))\n",
        "plt.plot(epochs, acc, 'r', label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', label=\"Validation Accuracy\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig2 = plt.figure(figsize=(14,7))\n",
        "plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_DIR = \"/content/drive/MyDrive/New_data_set/Test/\"\n",
        "\n",
        "Test_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "\n",
        "Test_generator = Test_datagen.flow_from_directory(TEST_DIR,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(img_height, img_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Test_generator.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_dict=Test_generator.class_indices\n",
        "labels= Test_generator.labels\n",
        "file_names= Test_generator.filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(file_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss , test_acc = model.evaluate_generator( Test_generator , steps = 50)\n",
        "print(\"test accuracy ===>\" ,test_acc)\n",
        "print(\"test loss =======>\",test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# connect drive :)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_width=640; img_height=340\n",
        "batch_size=4\n",
        "\n",
        "# target_size = (hight, width, 3)\n",
        "# img.resize = (width, hight, channle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_width=500 ; img_height=500\n",
        "batch_size=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   shear_range = 0.2,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   fill_mode ='nearest'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAINING_DIR = \"/content/drive/MyDrive/Data_Food_Consumed/Train\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1/255.0,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   shear_range = 0.2,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   fill_mode ='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(img_height, img_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VALIDATION_DIR = \"/content/drive/MyDrive/Data_Food_Consumed/Vlidation\"\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              target_size=(img_height, img_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TEST_DIR = \"/content/drive/MyDrive/Data_Food_Consumed/Test\"\n",
        "\n",
        "Test_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "\n",
        "Test_generator = Test_datagen.flow_from_directory(TEST_DIR,\n",
        "                                                    batch_size=4,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(400, 400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr=ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=10 , verbose=1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.applications import InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model=InceptionV3(weights=\"imagenet\",include_top=False,input_shape=(340 ,640,3))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_trainable = False\n",
        "for layer in conv.layers:\n",
        "  if layer.name == \"conv5_block3_3_conv\":\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#      fine tunning         10  1401\n",
        "pre_trained_model=InceptionV3(weights=\"imagenet\",include_top=False,input_shape=(340 ,340,3))\n",
        "\n",
        "\n",
        "model=models.Sequential()\n",
        "model.add(pre_trained_model)\n",
        "  Freeze all layers in the pre-trained model\n",
        " for layer in pre_trained_model.layers:\n",
        "     layer.trainable = False\n",
        "     print(f\"Layer {layer.name} is trainable: {layer.trainable}\")  # Print trainable status\n",
        "\n",
        "set_trainable = False\n",
        "for layer in pre_trained_model.layers:\n",
        "  if layer.name == \"mixed5\":\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "model.add(layers.Flatten())\n",
        " model.add(layers.Dense(512, activation='relu'))\n",
        " model.add(layers.Dropout(0.2))\n",
        " model.add(layers.Dense(256, activation='relu'))\n",
        " model.add(layers.Dense(128, activation='relu'))\n",
        " model.add(layers.Dense(64, activation='relu'))\n",
        " model.add(layers.Dense(32 , activation=\"relu\"))\n",
        "\n",
        "# model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Dense(32, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer='Adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics =['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(train_generator,\n",
        "                              epochs=100 ,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator,\n",
        "                              callbacks = [reduce_lr]\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc))\n",
        "\n",
        "fig = plt.figure(figsize=(14,7))\n",
        "plt.plot(epochs, acc, 'r', label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', label=\"Validation Accuracy\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.image import imread\n",
        "import pathlib\n",
        "fig2 = plt.figure(figsize=(14,7))\n",
        "plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss , test_acc = model.evaluate_generator( Test_generator , steps = 50)\n",
        "\n",
        "print(\"test accuracy ===>\" , test_acc)\n",
        "print(\"test loss =======>\" , test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load model\n",
        "from keras.models import load_model\n",
        "model = load_model(\"/content/drive/MyDrive/Data_set/new_data_set/best_FINAL_cnn.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import models\n",
        "from PIL import Image\n",
        "from skimage.io import imread\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install efficientnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.applications import EfficientNetB7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.applications.efficientnet import EfficientNetB7\n",
        "conv= EfficientNetB7(weights=\"imagenet\",include_top=False,input_shape=(600 ,600,3))\n",
        "model=models.Sequential()\n",
        "model.add(conv)\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(32 , activation=\"relu\"))\n",
        "\n",
        "\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Dense(16, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer='Adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics =['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(train_generator,\n",
        "                              epochs=6 ,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator,\n",
        "                              callbacks = [reduce_lr]\n",
        "                              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EffecienNet from keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c\n",
        "#IMG_SIZE = 600\n",
        "\n",
        "def build_model(num_classes):\n",
        "    inputs = layers.Input(shape=(450, 450, 3))\n",
        "    x = inputs\n",
        "    model = EfficientNetB7(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
        "\n",
        "    # Freeze the pretrained weights\n",
        "    model.trainable = True\n",
        "\n",
        "    # Rebuild top\n",
        "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    top_dropout_rate = 0.2\n",
        "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "    Dense1 = layers.Dense(256, activation=\"relu\")(x)\n",
        "    Dense2 = layers.Dense(128, activation=\"relu\")(Dense1)\n",
        "    Dense3 = layers.Dense(64, activation=\"relu\")(Dense2)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(Dense3)\n",
        "\n",
        "    # Compile\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    print(\"Device:\", tpu.master())\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
        "    strategy = tf.distribute.MirroredStrategy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_hist(hist):\n",
        "    plt.plot(hist.history[\"accuracy\"])\n",
        "    plt.plot(hist.history[\"val_accuracy\"])\n",
        "    plt.title(\"model accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model = build_model(num_classes=32)\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 20  # @param {type: \"slider\", min:8, max:80}\n",
        "hist = model.fit(train_generator, epochs=epochs, validation_data=validation_generator, verbose=1)\n",
        "plot_hist(hist)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
